# üõçÔ∏è Modern Data Platform: Automated ELT with Dagster, dbt, & DuckDB

![CI/CD Pipeline](https://github.com/KMoex-HZ/modern-data-platform-dagster/actions/workflows/ci_cd.yml/badge.svg)
![dbt Core](https://img.shields.io/badge/dbt-1.8-FF694B?logo=dbt&logoColor=white)
![DuckDB](https://img.shields.io/badge/DuckDB-1.0-FFF000?logo=duckdb&logoColor=black)
![Dagster](https://img.shields.io/badge/Orchestration-Dagster-black?logo=dagster)
![Soda Core](https://img.shields.io/badge/Quality-Soda_Core-4B32C3)

A production-grade **Modern Data Stack (MDS)** implementation designed to ingest, transform, and validate analytical data with strict quality guardrails.

This project demonstrates a **Data Engineering Lifecycle** focusing on **SCD Type 2 (History Tracking)**, **Automated Quality Checks**, and **CI/CD Integration**, simulating a real-world environment for a financial or retail institution.

---

## üìä Project Scope & Highlights

- **Goal:** Build a robust ELT pipeline that handles raw transactional data and transforms it into business-ready dimensional models.
- **Scale:** Optimized for high-performance local analytics using **DuckDB** (Columnar Storage), capable of processing millions of rows efficiently without heavy infrastructure overhead.
- **Engineering Standards:** Implements modular architecture, environment isolation (Docker/Virtualenv), and "Infrastructure as Code" principles.

---

## üèóÔ∏è Architecture

The pipeline follows a declarative **Asset-Based Orchestration** pattern.

```mermaid
%%{init: {'theme': 'base', 'themeVariables': { 'primaryColor': '#ffcc00', 'edgeLabelBackground':'#ffffff', 'tertiaryColor': '#fff'}}}%%
graph LR
    subgraph Ingestion ["Flux: Ingestion Layer"]
        direction TB
        A1[("Python Generator<br>(Faker Library)")]
        A2[("DuckDB<br>Raw Tables")]
        A1 -->|SQLAlchemy| A2
    end

    subgraph Transformation ["dbt Transformation Layer"]
        direction TB
        B1("Staging Views<br>(Clean & Cast)")
        B2("Snapshots<br>(SCD Type 2 History)")
        B3("Marts<br>(Fact & Dim Tables)")

        A2 --> B1
        B1 --> B2
        B1 --> B3
        B2 --> B3
    end

    subgraph Quality ["Quality Guardrails"]
        direction TB
        C1{dbt Native Tests}
        C2{Soda Core Checks}

        B3 --> C1
        B3 --> C2
    end

    subgraph Orchestration ["Orchestrator"]
        D1[Dagster Assets]
        D1 -.->|Triggers| Ingestion
        D1 -.->|Triggers| Transformation
    end

    C1 -->|Pass| End[Ready for BI]
    C2 -->|Pass| End
    C1 -->|Fail| Alert[Pipeline Failure]
    C2 -->|Fail| Alert

    style Ingestion fill:#e3f2fd,stroke:#1565c0,stroke-width:2px
    style Transformation fill:#fff3e0,stroke:#e65100,stroke-width:2px
    style Quality fill:#fce4ec,stroke:#c2185b,stroke-width:2px

```

_(Figure 1: End-to-End Architecture rendered dynamically via Mermaid.js)_

### Data Flow Visualization

The following lineage graph illustrates the data transformation journey from raw source tables to final business marts, including the snapshot layer for history tracking.

<div align="center">
  <img src="assets/dbt-lineage.png" alt="dbt Lineage Graph" width="80%">
</div>

_(Figure 2: The dbt Lineage Graph generated from the project manifest)_

### Component Explanation:

1. **Ingestion:** Python scripts generate synthetic transactional data and load it into **DuckDB** using SQLAlchemy for stability in CI environments.
2. **Orchestration:** **Dagster** manages the dependency graph and materialization schedules.
3. **Transformation (dbt):**

- **Staging:** Renaming and type casting raw data.
- **Snapshots:** Handling **Slowly Changing Dimensions (SCD Type 2)** to track user address changes over time (`valid_from`, `valid_to`).
- **Marts:** Creating `fact_orders` and `dim_users` for downstream BI tools.

4. **Quality Assurance:**

- **dbt Tests:** Internal logic validation (e.g., ensuring order amounts are positive).
- **Soda Core:** External data contracts (e.g., schema validation, missing value checks).

---

## üìÅ Repository Structure

```text
modern-data-platform-dagster/
‚îú‚îÄ‚îÄ .github/workflows/       # Automated CI/CD & Data Quality Guardrails (GitHub Actions)
‚îÇ   ‚îî‚îÄ‚îÄ ci_cd.yml            # Environment-agnostic pipeline for testing & validation
‚îú‚îÄ‚îÄ assets/                  # Documentation artifacts & visualization
‚îÇ   ‚îî‚îÄ‚îÄ dbt-lineage.png      # High-level DAG of the transformation lifecycle
‚îú‚îÄ‚îÄ dagster_project/         # Asset-Based Orchestration & Ingestion Layer
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py          # Dagster asset definitions and orchestration metadata
‚îÇ   ‚îî‚îÄ‚îÄ fake_data_gen.py     # Synthetic transaction engine (SQLAlchemy + Faker)
‚îú‚îÄ‚îÄ dbt_project/             # Analytics Engineering Layer (SCD Type 2 & Modeling)
‚îÇ   ‚îú‚îÄ‚îÄ models/              # Modular SQL logic (Staging, Snapshots, & Marts)
‚îÇ   ‚îú‚îÄ‚îÄ snapshots/           # Slowly Changing Dimensions (SCD Type 2) history tracking
‚îÇ   ‚îú‚îÄ‚îÄ tests/               # Custom business logic & data integrity assertions
‚îÇ   ‚îú‚îÄ‚îÄ dbt_project.yml      # Transformation metadata & project configuration
‚îÇ   ‚îî‚îÄ‚îÄ profiles.yml         # Dynamic connection profiles (Env var-driven for portability)
‚îú‚îÄ‚îÄ soda/                    # Data Contract & Governance Layer
‚îÇ   ‚îú‚îÄ‚îÄ checks.yml           # Declarative quality thresholds (Soda Checks)
‚îÇ   ‚îî‚îÄ‚îÄ configuration.yml    # Data source connectivity with path-injection placeholder
‚îú‚îÄ‚îÄ docker-compose.yml       # Containerized environment orchestration
‚îú‚îÄ‚îÄ requirements.txt         # Production-grade Python dependencies
‚îî‚îÄ‚îÄ README.md                # Technical documentation & architecture deep-dive
```

---

## üõ†Ô∏è Quick Start Guide

Follow these steps to deploy the data platform locally:

### 1. Clone the Repository

```bash
git clone [https://github.com/KMoex-HZ/modern-data-platform-dagster.git](https://github.com/KMoex-HZ/modern-data-platform-dagster.git)
cd modern-data-platform-dagster

```

### 2. Set Up Environment

```bash
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
pip install -r requirements.txt

```

### 3. Run the Pipeline

```bash
# 1. Ingest Data
python dagster_project/fake_data_gen.py

# 2. Transform Data & Track History
cd dbt_project
dbt deps
dbt snapshot
dbt run

# 3. Run Quality Checks
dbt test
cd ..
soda scan -d analytics -c soda/configuration.yml soda/checks.yml

```

---

## ‚ö†Ô∏è Production Considerations

This project is built with **Production-Grade** standards in mind:

1. **Ingestion Strategy:** Currently uses `SQLAlchemy` for CI/CD compatibility. For high-volume production datasets (>10GB), the pipeline is designed to switch to DuckDB's native `COPY` command or `read_parquet` for maximum I/O throughput.
2. **Environment Isolation:** The `profiles.yml` utilizes environment variables (`DUCKDB_PATH`), ensuring code portability between Local Development, Docker Containers, and GitHub Actions runners without code changes.
3. **Security:** No credentials are hardcoded. Database passwords and paths are managed via `.env` files and GitHub Secrets.

---

**Author:** Caelan Zhou

**Role:** Data Engineer / Data Science Student @ ITERA

**Location:** Bandar Lampung, Indonesia

**License:** MIT
